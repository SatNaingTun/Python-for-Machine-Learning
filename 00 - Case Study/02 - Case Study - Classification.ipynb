{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Classification\n",
    "\n",
    "This dataset is a **classification problem**, concerning whether if the loan was approved or not.\n",
    "\n",
    "- **Loan_ID** - unique loan ID\n",
    "- **Gender** - male/female\n",
    "- **Married** - yes/no\n",
    "- **Dependents** - number of dependents\n",
    "- **Education** - graduate/undergraduate\n",
    "- **Self_Employed** - yes/no\n",
    "- **ApplicantIncome** - applicant income\n",
    "- **CoapplicantIncome** - coapplicant income\n",
    "- **LoanAmount** - loan amount in thousands\n",
    "- **Loan_Amount_Term** - term of lean in months\n",
    "- **Credit_History** - credit history meets guidelines\n",
    "- **Property_Area** - Urban/semi and Rural\n",
    "- **Loan_Status** - approved yes/no \n",
    "\n",
    "We are given 2 set, the training set and the test set.\n",
    "\n",
    "The **training set** contains 614 samples and 13 features, 12 of which are the independent variables and the last feature `Loan_Status` is the dependent variable.\n",
    "\n",
    "The **test set** contains 367 samples with the same 12 features but without the `Loan_Status` columns. So it will be representing the unseen data that we will be implementing our model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data from google classroom\n",
    "#kaggle competition\n",
    "df_train = pd.read_csv(\"data/train_LoanPrediction.csv\")\n",
    "df_test  = pd.read_csv(\"data/test_LoanPrediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. get the shape (rows, columns)\n",
    "df_train.shape  #(614 rows, 13 columns)\n",
    "\n",
    "#2. try .head()\n",
    "df_train.head()\n",
    "# df_train.info()\n",
    "\n",
    "#3. try .describe()\n",
    "# df_train.describe()\n",
    "\n",
    "#4. check whether our data class is balanced, using value_counts(normalize=True)\n",
    "# df_train['Loan_Status'].value_counts() #if you don't put normalize=True, will give you raw number\n",
    "\n",
    "#our class is imbalanced....\n",
    "#it means, we have to downsample Y, to equal N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['ApplicantIncome'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condMaxAI = df_train['ApplicantIncome'] == 81000\n",
    "df_train[condMaxAI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[409]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Loan_Status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the class\n",
    "\n",
    "- We need to balance the label in the training set, because imbalanced class affects the model during training\n",
    "- If we do not balance the class, you can also try look at the SMOTE (https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html) algorithm which dynamically augments data on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. set the cond = Y, and cond = N\n",
    "condY = df_train.Loan_Status == 'Y'  #to 0 or 1\n",
    "condN = df_train.Loan_Status == 'N'\n",
    "\n",
    "#2.. df[condY].sample(n = 192)\n",
    "#    df[condN].sample\n",
    "#    we do this many times, do whole ML thing, and do the average....\n",
    "df_trainY = df_train[condY].sample(n=192, random_state=999)\n",
    "df_trainN = df_train[condN] #also 192\n",
    "\n",
    "#3. concat these two dfs\n",
    "df_train = pd.concat([df_trainY, df_trainN])\n",
    "\n",
    "df_train.Loan_Status.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding\n",
    "\n",
    "This is a useful step to do before EDA, so that categories are turned into numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do label encoding the Loan Status\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_train['Loan_Status'] = le.fit_transform(df_train['Loan_Status'])\n",
    "\n",
    "#very interesting...our testing set has no loan status, because this is actually a real testing set....\n",
    "\n",
    "print(\"I want to know the mapping of 0 and 1\", le.inverse_transform([0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encode education as well\n",
    "df_train['Education'] = le.fit_transform(df_train['Education'])\n",
    "df_test['Education']  = le.transform(df_test['Education'])\n",
    "\n",
    "#optionally you can use pd.map\n",
    "#df_train.map({'Graduate': 0, 'Not Graduate': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Property_Area.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why one-hot encode Property Area, not encode it into 0, 1, 2\n",
    "#if we have more than two categories, if we encode into 0, 1, 2\n",
    "#we create a unintentional order, i.e., the model \"may\" think that 0 < 1 < 2\n",
    "\n",
    "#what if we have like 5000 categories....\n",
    "    #one hot encode this will result in 5000 columns --> this introduces burden to the machine learning model\n",
    "    #1. NLP --> we basically do like this....\n",
    "    #2. General case -->\n",
    "        #1st:  Group these categories into bigger categories, and then one-hot encode\n",
    "        #2nd:  (not so good) - do label encoding anyway......but note the possible order effect\n",
    "        \n",
    "#one hot encoding\n",
    "#[1, 0, 0], [0, 1, 0], [0, 0, 1]\n",
    "#one thing you need to know is that you can always cut down one column\n",
    "#[1, 0], [0, 1], [0, 0]  ==> this is possible, and will save you one more column...\n",
    "#also this is recommended, because it will not introduce unwanted correlation.....\n",
    "#this is done by setting \"drop_first=True\"\n",
    "\n",
    "df_train = pd.get_dummies(df_train, columns=['Property_Area'], drop_first=True)\n",
    "df_test  = pd.get_dummies(df_test,  columns=['Property_Area'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0. Check the data type\n",
    "df_train.dtypes\n",
    "df_train.drop(columns = ['Loan_ID'], inplace=True)  #this is not a useful feature....\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "#1. You want to divide into categorical and numerical columns using select_dtypes\n",
    "df_train.dtypes\n",
    "num_col = df_train.select_dtypes(include=['int64', 'float64'])\n",
    "# numcol = numcol.drop([]) #Loan_Status\n",
    "cat_col = df_train.select_dtypes(exclude=['int64', 'float64'])\n",
    "num_col.columns, cat_col.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. for numerical type, lets plot some a bar plot with Loan Status\n",
    "for col in num_col.columns:\n",
    "    sns.barplot(x = df_train['Loan_Status'], y = df_train[col])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countplot / Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. for categorial type, you may want to use countplot with Loan Status\n",
    "for col in cat_col.columns:\n",
    "    sns.countplot(x = df_train[col], hue = df_train['Loan_Status'])\n",
    "    #similar to hue in scatterplot\n",
    "    plt.show()\n",
    "    # sns.countplot()  #because categorical, you need to count, not simply use the magnitude...so cannot use bar..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_train.corr(), annot=True)  #this is only for numeric values...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Power Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ppscore as pps\n",
    "\n",
    "# before using pps, let's drop country and year\n",
    "dfcopy = df_train.copy()\n",
    "\n",
    "#this needs some minor preprocessing because seaborn.heatmap unfortunately does not accept tidy data\n",
    "matrix_df = pps.matrix(dfcopy)[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection\n",
    "\n",
    "Since our dataset is already splitted at the dataset level, thus we do not need to hurry to select the features.  We can preprocess and then select later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#forward selection - process of starting with one feature, and slowly adding one feature at a time, until the performance does not improve by certain threshold you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing\n",
    "\n",
    "### Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. please check the missing value in df_train, and of course df_test\n",
    "df_train.isna().sum()\n",
    "\n",
    "#Gender:            majority or ratio\n",
    "#Dependents:        majority or ratio\n",
    "#Self_Employed:     majority or ratio\n",
    "#LoanAmount:        median or mean\n",
    "#Loan_Amount_Term:  majority or ratio\n",
    "#Credit_History:    majority or ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a missing categorical feature, we can replace with three things:\n",
    "- mode (most frequent) --> bad\n",
    "- ratio (keep the ratio intact) --> preferred\n",
    "- \"No category\" --> believe replacing is not a good option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = df_train['Credit_History'].value_counts(normalize=True)\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #either replace with the majority or keep the ratio\n",
    "# #majority is good when the majority takes over like 90% of the population\n",
    "# #otherwise, ratio is better\n",
    "\n",
    "# #in this case, it's better to fill the missing value with ratio\n",
    "# #ratio means that we want to keep the ratio 75 25 AFTER filling....\n",
    "print(\"Credit history of 1: \", ratio[1])\n",
    "print(\"Credit history of 0: \", ratio[0])\n",
    "print(ratio.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #we gonna fill the missing value such that the ratio is kept unchanged\n",
    "# #format: fillna(pd.Series(random.choice(1, 0), p=probability of the ratios), inplace=True)\n",
    "missing = df_train.Credit_History.isna().sum()\n",
    "\n",
    "num1 = int(np.round(ratio[1] * missing))\n",
    "num0 = int(np.round(ratio[0] * missing))\n",
    "\n",
    "print(num1, num0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Credit_History'].fillna(1.0, limit = num1, inplace=True)\n",
    "df_train['Credit_History'].fillna(0.0, limit = num0, inplace=True)\n",
    "\n",
    "#pd.series is ok, but i feel like limit is much easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Credit_History'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the missing for the test set for credit history\n",
    "\n",
    "#number of missing values\n",
    "missing = df_test['Credit_History'].isna().sum()\n",
    "\n",
    "#get the row index where the missing values located at\n",
    "cond = df_test['Credit_History'].isna()  #list of true and false\n",
    "missing_index = df_test[cond].index\n",
    "\n",
    "#get ratio\n",
    "ratio = df_test['Credit_History'].value_counts(normalize=True)\n",
    "print(\"Test ratio: \", ratio)\n",
    "\n",
    "series = pd.Series(np.random.choice((1, 0), p=[ratio[1], ratio[0]], size=missing), index=missing_index)\n",
    "print(series.value_counts(normalize=True))\n",
    "\n",
    "print(\"Before: \", df_test['Credit_History'].isna().sum())\n",
    "print(\"Before: \", df_test['Credit_History'].value_counts(normalize=True))\n",
    "\n",
    "df_test['Credit_History'].fillna(series, inplace=True)\n",
    "df_test['Credit_History'].isna().sum()\n",
    "\n",
    "print(\"After: \", df_test['Credit_History'].isna().sum())\n",
    "print(\"After: \", df_test['Credit_History'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_train.LoanAmount)\n",
    "print(f\"median: {df_train.LoanAmount.median()}; mean: {df_train.LoanAmount.mean()}\")\n",
    "\n",
    "#can you guys help me fill in the LoanAmount with median()\n",
    "df_train['LoanAmount'].fillna(df_train['LoanAmount'].median(), inplace=True)\n",
    "\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I stop because I am too lazy as the rest of the features won't be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StandardScaler is usually used for continuous values\n",
    "#here, Education, Credit_History, and Property Area are all categorical\n",
    "#standardizing them will lose the category, so we will not do!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. set the X and y\n",
    "X_train = df_train[ ['Education', 'Credit_History', 'Property_Area_Semiurban', 'Property_Area_Urban'] ]\n",
    "y_train = df_train['Loan_Status']\n",
    "\n",
    "X_test = df_test[ ['Education', 'Credit_History', 'Property_Area_Semiurban', 'Property_Area_Urban'] ]\n",
    "#no y_test, because this dataset does not have the answer....\n",
    "\n",
    "#2. make sure the shape of X is (samples, features), and y is (samples, )\n",
    "assert X_train.ndim == 2\n",
    "assert X_test.ndim  == 2\n",
    "assert y_train.ndim == 1\n",
    "\n",
    "print(\"(samples, features): \", X_train.shape)\n",
    "print(\"(samples,         ): \", y_train.shape)\n",
    "print(\"(samples, features): \", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. perform cross validation with a selected set of models, to scope down which model is among the best\n",
    "\n",
    "#3.1 specify the list of algorithms\n",
    "#this is for classification\n",
    "##Baselines\n",
    "from sklearn.linear_model import LogisticRegression  #drawing a line based on linear regression but used for classification\n",
    "from sklearn.naive_bayes import GaussianNB  #drawing a line based on probability\n",
    "\n",
    "##Situational (but we don't use much)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "##Complex\n",
    "from sklearn.ensemble import RandomForestClassifier  #using trees to classify\n",
    "from sklearn.svm import SVC  #drawing a line based on maximum distance\n",
    "from sklearn.ensemble import GradientBoostingClassifier  #<<-------is the MOST complex\n",
    "    #XGBoost but is not in sklearn, import XGBoost #<---most powerful algorithm for tabular data (not images, signal)\n",
    "\n",
    "#any random_state you can use......up to you \n",
    "lr = LogisticRegression(random_state=999)\n",
    "rf = RandomForestClassifier(random_state=999)\n",
    "sv = SVC(random_state=999)\n",
    "\n",
    "models = [lr, rf, sv]\n",
    "\n",
    "#3.2 perform cross validation using KFold\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "kfold = KFold(n_splits = 5, shuffle = True, random_state=999)\n",
    "\n",
    "for model in models:\n",
    "    score = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')  #f1, recall, precision, accuracy\n",
    "    print(\"Scores: \", score, \"- Scores mean: \", score.mean(), \"- Scores std (lower better): \", score.std())  #out of 1 ; 1 means perfect accuracy\n",
    "    #lr, rf, sv\n",
    "\n",
    "#when they are very close, \n",
    "    #how we choose?  we choose the simplest model...\n",
    "    #what does simplest mean?  LogisticRegression is simplest, and SVM is the most complex....\n",
    "    \n",
    "#overfitting vs. underfitting\n",
    "#overfitting means your model learns too much about the training set, \n",
    "    #so it cannot generalize to validation set or testing set (unseen data)\n",
    "    #is NOT good - it means your model CANNOT GENERALIZE to unseen data (validation or test set)\n",
    "    \n",
    "#underfitting means your model just fail to see any patterns in the training set\n",
    "    #this is easy: just look at score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. grid search to find the best version of that model\n",
    "#remind you: grid search is BASICALLY same as cross validation but for the same model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = LogisticRegression(random_state=999)  #<----this is the model I choose, after cross validation\n",
    "\n",
    "param_grid = dict()\n",
    "param_grid['solver'] = ['newton-cg', 'lbfgs', 'liblinear']  #this is listed in the sklearn website\n",
    "\n",
    "#add more parameters here\n",
    "#param_grid[parameter] = list of parameters to search\n",
    "\n",
    "#refit means it will pick the best model, and fit again, so it means grid is already the best model after this line\n",
    "grid = GridSearchCV(model, param_grid, scoring=\"accuracy\", cv=kfold, refit=True, return_train_score=True)\n",
    "#scoring = f1, recall, precision, accuracy\n",
    "\n",
    "#fit the grid, which will basically do cross validation across all combinatiosn, here we only have 3 comb\n",
    "grid.fit(X_train, y_train)  #remember to use only training set here....\n",
    "\n",
    "#print the best parameters and accuracy\n",
    "# print(grid.best_params_)\n",
    "# print(grid.best_score_)\n",
    "print(grid.cv_results_)\n",
    "\n",
    "#this score is cross-validation score, basically the accuracy/precision/etc on the validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification metrics\n",
    "\n",
    "Let us study some classification metrics that are quite different from the $r^2$ or mean squared error that we see from the regression.  Let me define a confusion matrix that looks like this:\n",
    "\n",
    "<code>\n",
    "\t\t \t    Actual\n",
    "\t\t\t    +\t   -\n",
    "Predicted +     TP     FP\n",
    "          -     FN     TN\n",
    "</code>\n",
    "\n",
    "TP is defined as true positives, FP as false positives, FN as false negatives, and TN as true negatives.\n",
    "\n",
    "#### Accuracy, Recall, Precision, F1\n",
    "\n",
    "Accuracy is straightforward\n",
    "\n",
    "$$ \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} $$\n",
    "\n",
    "**Accuracy** is mostly avoided, unless your model is really balanced of both positives and negatives.  Instead, more useful classification metrics would be precision, recall, and f1-score\n",
    "\n",
    "$$ \\text{Precision} = \\text{TP} / (\\text{TP} + \\text{FP}) $$\n",
    "\n",
    "**Precision** is useful as metric when you want to prioritize removing false positive.  Example is search engine in which you do not want to return any search results that are \"false positive\"\n",
    "\n",
    "$$ \\text{Recall} = \\text{TP} / (\\text{TP} + \\text{FN}) $$\n",
    "\n",
    "**Recall** is useful as metric when you want to prioritize removing false negative.  Example is cancer detection in which you do not want to miss detecting any real positive (i.e., false negative).\n",
    "\n",
    "$$ \\text{F1} = 2 x \\frac{\\text{Precision} * \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\n",
    "\n",
    "**F1** is simply seeking a balance between Precision and Recall.  Also F1 is good metric when there is an uneven class distribution (large number of actual negatives)\n",
    "\n",
    "sklearn provides a easy-to-use <code>**sklearn.metrics.classification_report**.</code> API that report these four metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix \n",
    "'''\n",
    "T = True, F = False\n",
    "P = Positive, N = Negative\n",
    "'''\n",
    "\n",
    "''' \n",
    "                        Actual\n",
    "                    P               N\n",
    "             P     TP              FP\n",
    "predict   \n",
    "             N     FN              TN\n",
    "'''\n",
    "\n",
    "#0 here is negative\n",
    "#1 here is positive\n",
    "\n",
    "y     = [0, 0, 1, 1, 0, 1]\n",
    "ypred = [1, 0, 1, 1, 0, 1]\n",
    "\n",
    "#accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
    "#accuracy = 3 + 2  /  6 = 5 / 6 = 83%\n",
    "\n",
    "#but accuracy is NOT good if your data is imbalanced (see actual y)\n",
    "\n",
    "#recall, precision, f1-score\n",
    "#recall = TP  / (TP + FN)\n",
    "#recall = 3   / (3  + 0) = 100%  Recall is used when you don't like FN, e.g., critical - cancer, zombie, terrorists\n",
    "\n",
    "#precision = TP / (TP + FP)\n",
    "#precision = 3 /  (3 + 1) = 75% Precision is used when you don't like FP, e.g., search engine\n",
    "\n",
    "#f1-score = 2 * ( (recall * precision) / (precision + recall) )\n",
    "        #when precision and recall are both important.....\n",
    "        #   2 * ( (1      * 0.75)     /   (0.75     +    1  ) )\n",
    "        \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#if you want all in one table\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#if you want the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "#format (ytrue, ypred)\n",
    "print(accuracy_score(y, ypred))\n",
    "print(precision_score(y, ypred))\n",
    "print(recall_score(y, ypred))\n",
    "print(f1_score(y, ypred))\n",
    "\n",
    "print(classification_report(y, ypred))\n",
    "\n",
    "cm = confusion_matrix(y, ypred)\n",
    "cmp = ConfusionMatrixDisplay(cm, display_labels=[0, 1])\n",
    "\n",
    "cmp.plot()\n",
    "\n",
    "#for regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explain the classification report a bit:\n",
    "\n",
    "- <code>weight_avg</code> mutiplies score with the number of true labels in that class, favoring the majority label\n",
    "#macro simply average, thus better for imbalanced dataset\n",
    "\n",
    "- <code>macro_avg</code> \n",
    "\n",
    "- <code>support</code> refers to number of samples belonging to that class\n",
    "\n",
    "Between precision or recall or f1?\n",
    "- Use **recall** for dangerous stuffs - healthcare, security, fraud. You don't care about false positives too much (i.e., false alarm) because you want to always stay fully cautious.\n",
    "\n",
    "- Use **precision** for search/identifying something - HR, search engine, etc.  You care a lot about false positive, i.e., you want to identify really good sample.\n",
    "\n",
    "- Use **f1-score** - basically a good bet for almost all imbalanced dataset.\n",
    "\n",
    "Final recommendations:\n",
    "- in balanced dataset, just simply use <code>accuracy</code>\n",
    "- in imbalanced dataset, use <code>macro avg f1-score</code> or <code>macro precision</code> or <code>macro recall</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = grid.predict(X_test)\n",
    "\n",
    "print(pred_y)\n",
    "\n",
    "#if we have y_test\n",
    "#precision_score(y_test, pred_y)\n",
    "\n",
    "#three cases (assume I have y_test):\n",
    "    #if I got score much lower than 0.7, e.g., 0.5 0.4, i have serious overfitting issue\n",
    "    #if I got score near 0.7 or just a little lower, or just a little higher, like 0.65 0.6 0.75, i think its ok\n",
    "    #if I got score like much much higher than 0.7, like 0.95, something is wrong.....maybe your test set is broken or something\n",
    "        #but this case rarely happens!\n",
    "        \n",
    "#overfitting means your model CANNOT GENERALIZE to unseen data....your unseen data can be validation or test set...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I don't really have the ground truth, I don't know the goodness of my model.  But let's assume I make some fake ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_y_test = np.random.randint(0, 2, size = (pred_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(fake_y_test, pred_y)  #precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analysis: Feature Importance\n",
    "\n",
    "- Basically understanding which features are important for prediction\n",
    "- Different algorithms have different way for feature importance\n",
    "- For Logistic Regression, similar to Linear Regression, you can look at the coeffients/weights/slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.coef_\n",
    "#['Education', 'Credit_History', 'Property_Area_Semiurban', 'Property_Area_Urban']\n",
    "\n",
    "#predict LoanStatus, LoanAmount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(['Education', 'Credit_History', 'Property_Area_Semiurban', 'Property_Area_Urban'], \n",
    "                                  columns=['features'])\n",
    "feature_importance[\"importance\"] = grid.best_estimator_.coef_[0]\n",
    "\n",
    "feature_importance = feature_importance.sort_values(by = ['importance'], ascending=True)\n",
    "\n",
    "feature_importance.plot.barh(x='features', y='importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model\n",
    "filename = 'model/Loan_Prediction.pkl' # pkl does not matter, you can do .everything\n",
    "pickle.dump(grid, open(filename,'wb'))\n",
    "\n",
    "# Load the model\n",
    "loaded_grid=pickle.load(open(filename,'rb'))\n",
    "\n",
    "# if you have new data, then you fit again....but using loaded_grid\n",
    "# which is a process of training more.....once you have more data....\n",
    "\n",
    "# or another way is\n",
    "# put all the dataset together, and train like it is new\n",
    "    #this is possible ONLY if your dataset is not that big......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try to create one silly example\n",
    "df_train[['Education', 'Credit_History', 'Property_Area_Semiurban', 'Property_Area_Urban', 'Loan_Status']].loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.array([[0, 1, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_loan_status= loaded_grid.predict(sample)\n",
    "predicted_loan_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's wrong!  So certainly we need to improve the model :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
