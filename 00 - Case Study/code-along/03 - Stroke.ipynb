{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data from google classroom\n",
    "#kaggle competition\n",
    "df = pd.read_csv(\"../data/brain_stroke.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stroke.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. set the cond = Y, and cond = N\n",
    "cond0 = df.stroke == 0  #to 0 or 1\n",
    "cond1 = df.stroke == 1\n",
    "\n",
    "df_0 = df[cond0].sample(n=248, random_state=999)\n",
    "df_1 = df[cond1] #also 192\n",
    "\n",
    "#3. concat these two dfs\n",
    "df = pd.concat([df_0, df_1])\n",
    "\n",
    "df.stroke.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding\n",
    "\n",
    "This is a useful step to do before EDA, so that categories are turned into numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gender = 2\n",
    "#ever_married = 2\n",
    "#work_type = got nan!!\n",
    "#Residence_type = got 2 + nan\n",
    "#smoking_status = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender.unique(), df.ever_married.unique(), df.Residence_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Residence_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do label encoding the Loan Status\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['gender'] = le.fit_transform(df['gender'])\n",
    "df['ever_married'] = le.fit_transform(df['ever_married'])\n",
    "df['Residence_type'] = le.fit_transform(df['Residence_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender.unique(), df.ever_married.unique(), df.Residence_type.unique()\n",
    "#I need to remember to clean up the 2 for Residence_type!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Residence_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.work_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.smoking_status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['work_type', 'smoking_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "#1. You want to divide into categorical and numerical columns using select_dtypes\n",
    "df.dtypes\n",
    "num_col = df.select_dtypes(include=['int64', 'float64'])\n",
    "# numcol = numcol.drop([]) #Loan_Status\n",
    "cat_col = df.select_dtypes(exclude=['int64', 'float64'])\n",
    "num_col.columns, cat_col.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. for numerical type, lets plot some a bar plot with Loan Status\n",
    "# for col in num_col.columns:\n",
    "#     plt.figure(figsize=(3, 2))\n",
    "#     if col != 'stroke':\n",
    "        # sns.barplot(x = df['stroke'], y = df[col])\n",
    "        # plt.show()\n",
    "        \n",
    "#so age, hypertension, heart_disease, ever_married, avg_glucose_level, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countplot / Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. for categorial type, you may want to use countplot with Loan Status\n",
    "# for col in cat_col.columns:\n",
    "#     plt.figure(figsize=(3, 2))\n",
    "#     sns.countplot(x = df[col], hue = df['stroke'])\n",
    "#     #similar to hue in scatterplot\n",
    "#     plt.show()\n",
    "    # sns.countplot()  #because categorical, you need to count, not simply use the magnitude...so cannot use bar...\n",
    "    \n",
    "#work_type_self_employed, children, unknown, formerly smoked, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 12))\n",
    "# sns.heatmap(df.corr(), annot=True)  #this is only for numeric values....\n",
    "#same trend as plots above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Power Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ppscore as pps\n",
    "\n",
    "# # before using pps, let's drop country and year\n",
    "# dfcopy = df.copy()\n",
    "\n",
    "# #this needs some minor preprocessing because seaborn.heatmap unfortunately does not accept tidy data\n",
    "# matrix_df = pps.matrix(dfcopy)[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\n",
    "\n",
    "# #plot\n",
    "# plt.figure(figsize = (15,8))\n",
    "# sns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=True)\n",
    "\n",
    "# same trend as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection\n",
    "\n",
    "Since our dataset is already splitted at the dataset level, thus we do not need to hurry to select the features.  We can preprocess and then select later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#forward selection - process of starting with one feature, and slowly adding one feature at a time, until the performance does not improve by certain threshold you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work_type_self_employed, children, unknown, formerly smoked, \n",
    "#so age, hypertension, heart_disease, ever_married, avg_glucose_level, \n",
    "\n",
    "X = df[ ['age', 'hypertension', 'heart_disease', 'ever_married', 'avg_glucose_level', 'work_type_Self-employed', 'work_type_children', 'smoking_status_formerly smoked']]\n",
    "y = df[ ['stroke']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing\n",
    "\n",
    "### Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. please check the missing value in df_train, and of course df_test\n",
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['avg_glucose_level'].fillna(X_train['avg_glucose_level'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "col_names = ['age', 'avg_glucose_level']\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train[['age']] = sc.fit_transform(X_train[['age']])\n",
    "X_test[['age']]  = sc.transform(X_test[['age']])\n",
    "\n",
    "X_train[['avg_glucose_level']] = sc.fit_transform(X_train[['avg_glucose_level']])\n",
    "X_test[['avg_glucose_level']]  = sc.transform(X_test[['avg_glucose_level']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. perform cross validation with a selected set of models, to scope down which model is among the best\n",
    "from sklearn.linear_model import LogisticRegression  #drawing a line based on linear regression but used for classification\n",
    "from sklearn.naive_bayes import GaussianNB  #drawing a line based on probability\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier  #using trees to classify\n",
    "from sklearn.svm import SVC  #drawing a line based on maximum distance\n",
    "from sklearn.ensemble import GradientBoostingClassifier  #<<-------is the MOST complex\n",
    "\n",
    "lr = LogisticRegression(random_state=999)\n",
    "rf = RandomForestClassifier(random_state=999)\n",
    "sv = SVC(random_state=999)\n",
    "\n",
    "models = [lr, rf, sv]\n",
    "\n",
    "#3.2 perform cross validation using KFold\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "kfold = KFold(n_splits = 5, shuffle = True, random_state=999)\n",
    "\n",
    "for model in models:\n",
    "    score = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')  #f1, recall, precision, accuracy\n",
    "    print(\"Scores: \", score, \"- Scores mean: \", score.mean(), \"- Scores std (lower better): \", score.std())  #out of 1 ; 1 means perfect accuracy\n",
    "    #lr, rf, sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. grid search to find the best version of that model\n",
    "#remind you: grid search is BASICALLY same as cross validation but for the same model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = LogisticRegression(random_state=999)  #<----this is the model I choose, after cross validation\n",
    "\n",
    "param_grid = dict()\n",
    "param_grid['solver'] = ['newton-cg', 'lbfgs', 'liblinear']  #this is listed in the sklearn website\n",
    "\n",
    "#add more parameters here\n",
    "#param_grid[parameter] = list of parameters to search\n",
    "\n",
    "#refit means it will pick the best model, and fit again, so it means grid is already the best model after this line\n",
    "grid = GridSearchCV(model, param_grid, scoring=\"accuracy\", cv=kfold, refit=True, return_train_score=True)\n",
    "#scoring = f1, recall, precision, accuracy\n",
    "\n",
    "#fit the grid, which will basically do cross validation across all combinatiosn, here we only have 3 comb\n",
    "grid.fit(X_train, y_train)  #remember to use only training set here....\n",
    "\n",
    "#print the best parameters and accuracy\n",
    "# print(grid.best_params_)\n",
    "# print(grid.best_score_)\n",
    "print(grid.cv_results_)\n",
    "\n",
    "#this score is cross-validation score, basically the accuracy/precision/etc on the validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred_y = grid.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analysis: Feature Importance\n",
    "\n",
    "- Basically understanding which features are important for prediction\n",
    "- Different algorithms have different way for feature importance\n",
    "- For Logistic Regression, similar to Linear Regression, you can look at the coeffients/weights/slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(['age', 'hypertension', 'heart_disease', 'ever_married', 'avg_glucose_level', 'work_type_Self-employed', 'work_type_children', 'smoking_status_formerly smoked'], \n",
    "                                  columns=['features'])\n",
    "feature_importance[\"importance\"] = grid.best_estimator_.coef_[0]\n",
    "\n",
    "feature_importance = feature_importance.sort_values(by = ['importance'], ascending=True)\n",
    "\n",
    "feature_importance.plot.barh(x='features', y='importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do by yourself!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
