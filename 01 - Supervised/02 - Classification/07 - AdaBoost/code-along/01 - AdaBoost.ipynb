{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../figures/ada2.png\" />\n",
    "\n",
    "## Hypothesis function\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h(\\mathbf{x}^{(i)}) & = \\text{sign}\\big(\\alpha_1h_1(\\mathbf{x}^{(i)}) + \\alpha_2h_2(\\mathbf{x}^{(i)}) + \\cdots + \\alpha_sh_s(\\mathbf{x}^{(i)}) )\\big) \\\\\n",
    "& = \\text{sign}\\big(\\sum_{s=1}^{S}\\alpha_sh_s(\\mathbf{x}^{(i)})\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define what is a good classifier\n",
    "\n",
    "\n",
    "Our job is to find the optimal $\\alpha_s$, so we can know which classifier we should give more weightage (i.e., believe more).  To get this alpha, we should first define what is \"good\" classifier.  This is simple, since good classifier should simply has the minimum weighted errors as:\n",
    "\n",
    "$$\\epsilon_s = \\frac{\\sum_{i=1}^m w_s^{(i)}I(h_s(\\mathbf{x}^{(i)}) \\neq y^{(i)})}{\\sum_{i=1}^m w_s^{(i)}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\text{range}(\\epsilon_s) = [0, 1]$$\n",
    "\n",
    "in which the weights are initialized in the beginning as\n",
    "\n",
    "$$w_s^{(i)} = \\frac{1}{m}$$ \n",
    "\n",
    "where\n",
    "\n",
    "$$\\sum_{i=1}^m w_s^{(i)} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, given $h(\\mathbf{x})$ as <code>yhat</code> and <code>y</code> as the real y, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate its weighted errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to change our weight bigger for the first one, you will see that the final error is enlarged.  (Please don't mind why it became 0.7 or 0.05; this is just example.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the weights\n",
    "\n",
    "Our goal is that once we got the error, we need to emphasize the incorrectly classified sample, so the next classifier will focus on making them right.  Thus we need a weight update rule.  The formula is as follows:\n",
    "\n",
    "$$w_{s+1}^{(i)} = w_s^{(i)}e^{ -\\alpha_sh_s(\\mathbf{x^{(i)}}) y^{(i)}}$$\n",
    "\n",
    "which then need to renormalize \n",
    "\n",
    "$$w_{s+1}^{(i)} = \\frac{w_{s+1}^{(i)}}{{\\displaystyle\\sum_{i=1}^m w_{s+1}^{(i)}}} $$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\\sum_{i=1}^m w_s^{(i)} = 1$$\n",
    "\n",
    "Here $\\alpha_s$ is:\n",
    "\n",
    "$$\\alpha_s = \\frac{1}{2}\\ln\\frac{1-\\epsilon_s}{\\epsilon_s}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\text{range}(\\alpha_s) = (-\\infty, \\infty)$$\n",
    "\n",
    "\n",
    "## Relationship between alpha and errors\n",
    "\n",
    "Here, higher the error, lower is alpha, which means we don't trust that classifier.  And vice versa.   If e is close to 0 (the classifier performs well), alpha will be positive, indicating that the classifier's predictions should have more influence on the final ensemble. If e is close to 0.5 (the classifier performs worst than random guessing), alpha will be negative, indicating that the classifier's predictions should have less influence, and they will be \"flipped\" in the ensemble.   Of course, if e is 1, then we should NOT even use this classifier!!\n",
    "\n",
    "First, to see why this formula works, let's plot alpha against errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this weight update rule works?\n",
    "\n",
    "$$w_{s+1}^{(i)} = w_s^{(i)}e^{ -\\alpha_sh_s(\\mathbf{x^{(i)}}) y^{(i)}}$$\n",
    "\n",
    "Let's first find the alpha.  Recall that:\n",
    "\n",
    "$$\\alpha_s = \\frac{1}{2}\\ln\\frac{1-\\epsilon_s}{\\epsilon_s}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\epsilon_s = \\frac{\\sum_{i=1}^m w_s^{(i)}I(h_s(\\mathbf{x}^{(i)}) \\neq y^{(i)})}{\\sum_{i=1}^m w_s^{(i)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we find the initial alpha, let's plug everthing into the weight update rule, starting from this component:\n",
    "\n",
    "$$h_s(\\mathbf{x^{(i)}}) y^{(i)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to understand what multiplying actually means.  Notice that negative means the answer is wrong.\n",
    "\n",
    "We multiply negative alpha so that incorrectly classified sample will have bigger value.\n",
    "\n",
    "Why do we need to make the incorrectly classified sample have bigger value?  Well, because we want incorrectly classified sample to have bigger weight so the next classifier can focus on it.\n",
    "\n",
    "$$-\\alpha_sh_s(\\mathbf{x^{(i)}}) y^{(i)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, since we will multiply this with the weight, we perform exp to make sure that the resulting number is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$e^{ -\\alpha_sh_s(\\mathbf{x^{(i)}}) y^{(i)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we calculate everything.\n",
    "\n",
    "$$w_{s+1}^{(i)} = w_s^{(i)}e^{ -\\alpha_sh_s(\\mathbf{x^{(i)}}) y^{(i)}}$$\n",
    "\n",
    "which then need to renormalize \n",
    "\n",
    "$$w_{s+1}^{(i)} = \\frac{w_{s+1}^{(i)}}{{\\displaystyle\\sum_{i=1}^m w_{s+1}^{(i)}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So what does this number means?**\n",
    "\n",
    "Well, notice that the incorrectly classified samples are #1 and #3, and they both have bigger weights than others.   This will make sure next classifier will focus on solving it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=500, random_state=1)\n",
    "y = np.where(y==0,-1,1)  #change our y to be -1 if it is 0, otherwise 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
