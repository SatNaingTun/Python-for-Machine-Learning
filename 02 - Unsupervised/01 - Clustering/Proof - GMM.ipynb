{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of GMM\n",
    "\n",
    "Assume $X = \\{x^{(i)}, \\cdots, x^{(m)}\\}$ are drawn form an unkown distribution $p(x)$.  Our objective is to find a good approximation of this unknown distribution by means of a GMM with $K$ mixture components.  We exploit our i.i.d (independently and identically distributed) assumption, which leads to the log-likelihood as\n",
    "\n",
    "$$\\log p(X | \\theta) = \\sum\\limits_{i=1}^m \\log p(x^{(i)} | \\theta) = \\sum\\limits_{i=1}^m \\log \\sum\\limits_{k=1}^K \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "Our objective function is to find $\\theta$ that maximize the log-likehood $\\mathcal{L}$\n",
    "\n",
    "$$\n",
    "\\max_\\theta \\sum\\limits_{i=1}^m \\log \\sum\\limits_{k=1}^K \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "Our \"normal\" procedure would be to comute the gradient $\\frac{d\\mathcal{L}}{d\\theta}$ of the log-likelihood with respect to the model parameters $\\theta$, set it to 0, and solve for $\\theta$, however, if you try this yourself at home, you will find that it is not possible to find the closed form.\n",
    "\n",
    "One way we can do turns out to be the EM algorithm, where the key idea is to update one model parameter at a time, while keeping the others fixed.\n",
    "\n",
    "Before we find the partial derivatives, let us introduce a quantity that will play a central role in this algorithm: **responsibilities**.\n",
    "\n",
    "We define the quantity\n",
    "\n",
    "$$ r^{(i)}_{k} = \\frac{\\pi_k\\mathcal{N}(x^{(i)} \\mid \\mu_k, \\Sigma_{k})}{\\Sigma_{j=1}^{K} \\pi_j\\mathcal{N}(x^{(i)} \\mid \\mu_j, \\Sigma_j)}$$\n",
    "\n",
    "as the *responsibility* of the $k$th mixture component for the $i$th data point.  \n",
    "\n",
    "$r^{(i)}_{k}$ basically gives us $$ \\frac{\\text{Probability of $x^{(i)}$ belonging to cluster k}}{\\text{Probability of $x^{(i)}$ over all clusters}} $$\n",
    "\n",
    "The responsibility $r^{(i)}_{k}$ of the $k$th mixture component for data point $x^{(i)}$ is proportional to the likelihood of the mixture component given the data point.\n",
    "\n",
    "$$p(x^{(i)} | \\pi_k, \\mu_k, \\Sigma_k) = \\pi_k\\mathcal{N}(x^{(i)}|\\mu_k, \\Sigma_k)$$\n",
    "\n",
    "Therefore, mixture components have a high responsibility for a data point when the data point could be a **plausible sample** from that mixture component.  Note that \n",
    "\n",
    "$$r^{(i)} = r^{(i)}_1, r^{(i)}_2, \\cdots, r^{(i)}_k \\in \\mathbb{R}^k$$\n",
    "\n",
    "is a normalized probability vector, i.e., for each sample $i$\n",
    "\n",
    "$$\\sum\\limits_{j=1}^{k}r^{(i)}_j = 1$$\n",
    "\n",
    "$$ r^{(i)}_j \\geq 0 $$\n",
    "\n",
    "Thus this probability vector distributes probability mass among the $K$ mixture components, and we can think of $r^{(ik)}$ as probability that $x^{(i)}$ has been generated by the $k$th mixture component.\n",
    "\n",
    "By summing all the total responsibility of the $k$th mixture component along all samples, we get $N_k$.\n",
    "\n",
    "$$N_k = \\sum\\limits_{i=1}^{m}r^{(i)}_k$$\n",
    "\n",
    "Note that this value does not necessarily equal to 1.\n",
    "\n",
    "**Updating the mean**\n",
    "\n",
    "The update of the mean parameters $\\mu_k, k=1,\\cdots,K$ of the GMM is given by:\n",
    "\n",
    "$$ \\mu_k^{new} = \\frac{\\sum\\limits_{i=1}^{m}r^{(i)}_{k}x^{(i)}}{\\sum\\limits_{i=1}^{m}r^{(i)}_{k}}$$\n",
    "\n",
    "To prove this:\n",
    "\n",
    "Any local optimum of a function exhibits the property that its gradient with respect to the parameters must vanish, i.e., setting its partial derivative to zero.\n",
    "\n",
    "We take a partial derivative of our objective function with respect to the mean parameters $\\mu_k, k=1, \\cdots, K$.  To simplify things, let's perform partial derivative without the log first and only consider one sample.\n",
    "\n",
    "$$\n",
    "\t\\frac{\\partial p(x^{(i)} | \\theta)}{\\partial \\mu_k} = \\sum\\limits_{j=1}^K \\pi_j \\frac{\\partial \\mathcal{N}(x^{(i)} | \\mu_j, \\Sigma_j)}{\\partial \\mu_k} = \\pi_k \\frac{\\partial \\mathcal{N}(x^{(i)} | \\mu_k, \\Sigma_k)}{\\partial \\mu_k} = \\pi_k(x^{(i)} - \\mu_k)^T \\Sigma_k^{-1}\\mathcal{N}(x^{(i)} | \\mu_k, \\Sigma_k)\n",
    "$$\n",
    "\n",
    "Now, taking all samples and log, since we know the partial derivative of $\\log$ something is $\\frac{1}{x}$, thus\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mu_k} =\\sum\\limits_{i=1}^{m} \\frac{\\partial \\log p(x^{(i)} | \\theta)}{\\partial \\mu_k} = \\sum\\limits_{i=1}^{m} \\frac{1}{p(x^{(i)} | \\theta)} \\frac{\\partial p(x^{(i)} | \\theta) }{\\partial \\mu_k} = \\sum\\limits_{i=1}^{m}(x^{(i)} - \\mu_k)^T \\Sigma_k^{-1}\\frac{\\pi_k\\mathcal{N}(x^{(i)} \\mid \\mu_k, \\Sigma_{k})}{\\Sigma_{j=1}^{K} \\pi_j\\mathcal{N}(x^{(i)} \\mid \\mu_j, \\Sigma_j)}\n",
    "$$\n",
    "\n",
    "To simplify, we can substitute $r^{(i)}_{k}$ into the equation, thus\n",
    "\n",
    "$$= \\sum\\limits_{i=1}^{m} r^{(i)}_{k}(x^{(i)} - \\mu_k)^T\\Sigma_k^{-1}$$\n",
    "\n",
    "We can now solve for $\\mu_k$ so that $\\frac{\\partial \\mathcal{L}}{\\partial \\mu_k} = 0$ and obtain\n",
    "\n",
    "$$\\sum\\limits_{i=1}^{m} r^{(i)}_{k}(x^{(i)} - \\mu_k)^T\\Sigma_k^{-1} = 0$$\n",
    "\n",
    "Multiply both sides by $\\Sigma$ will cancel out the inverse $\\Sigma$, and move $\\mu_k$ to another side\n",
    "\n",
    "$$\\sum\\limits_{i=1}^{m} r^{(i)}_{k}x^{(i)}  = \\sum\\limits_{i=1}^{m} r^{(i)}_{k}\\mu_k$$\n",
    "\n",
    "$$\\frac{\\sum\\limits_{i=1}^{m} r^{(i)}_{k}x^{(i)} }{\\sum\\limits_{i=1}^{m} r^{(i)}_{k}}  = \\mu_k$$\n",
    "\n",
    "We can further substitute $N_k$ so that\n",
    "\n",
    "$$\n",
    "\\frac{1}{N_k}\\sum\\limits_{i=1}^{m} r^{(i)}_{k}x^{(i)} = \\mu_k\n",
    "$$\n",
    "\n",
    "Here we can interpret that $\\mu_k$ is pulled toward a data point $x^{(i)}$ with strength given by $r^{(i)}_{k}$.  The means are pulled stronger toward data points for which the corresponding mixture component has a high responsibility, i.e., a high likelihood.\n",
    "\n",
    "**Updating the covariances**\n",
    "\n",
    "The update of the covariance parameters $\\Sigma_k, k=1,\\cdots,K$ of the GMM is given by:\n",
    "\n",
    "$$ \\Sigma_k^{new} = \\frac{1}{N_k} \\sum\\limits_{i=1}^{m}r^{(i)}_{k}(x^{(i)} - \\mu_k)(x^{(i)} - \\mu_k)^T $$\n",
    "\n",
    "To prove this:\n",
    "\n",
    "We take a partial derivative of our objective function with respect to the Sigma parameters $\\Sigma_k, k=1, \\cdots, K$.  Similarly, to simplify things, let's perform partial derivative without the log first and only consider one sample.\n",
    "\n",
    "$$\n",
    "\t\\frac{\\partial p(x^{(i)} | \\theta)}{\\partial \\Sigma_k} = \\frac{\\partial}{\\partial \\Sigma_k} \\big(\\pi_k(2\\pi)^{-\\frac{D}{2}} \\det(\\Sigma_k)^{\\frac{1}{2}}exp\\big(-\\frac{1}{2}(x^{(i)} - \\mu_k)^T\\Sigma^{-1}_k(x^{(i)} - \\mu_k)\\big)\\big)\n",
    "$$\n",
    "\n",
    "Using derivative multiplication rule, we got\n",
    "\n",
    "$$\n",
    "= \\pi_k(2\\pi)^{-\\frac{D}{2}}\\big[\\frac{\\partial}{\\partial \\Sigma_k}\\det(\\Sigma_k)^{-\\frac{1}{2}}exp\\big(-\\frac{1}{2}(x^{(i)} - \\mu_k)^T\\Sigma^{-1}_k(x^{(i)} - \\mu_k)) + \\det(\\Sigma_k)^{-\\frac{1}{2}}\\frac{\\partial}{\\partial \\Sigma_k}exp\\big(-\\frac{1}{2}(x^{(i)} - \\mu_k)^T\\Sigma^{-1}_k(x^{(i)} - \\mu_k)\\big]\n",
    "$$\n",
    "\n",
    "Using this following rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial X}\\det(f(x)) = \\det(f(x))tr\\big(f(x)^{-1}\\frac{\\partial f(x)}{\\partial x}\\big)\n",
    "$$\n",
    "\n",
    "We get that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Sigma_k}\\det(\\Sigma_k)^{-\\frac{1}{2}} = -\\frac{1}{2}\\det(\\Sigma_k)^{-\\frac{1}{2}}\\Sigma_k^{-1}\n",
    "$$\n",
    "\n",
    "Using this following rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a^TXb}{\\partial X} = ab^T\n",
    "$$\n",
    "\n",
    "We get that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Sigma_k}(x^{(i)} - \\mu_k)^T\\Sigma^{-1}_k(x^{(i)} - \\mu_k) = -\\Sigma_k^{-1}(x^{(i)} - \\mu_k)(x^{(i)} - \\mu_k)^T\\Sigma_k^{-1}\n",
    "$$\n",
    "\n",
    "Putting them together, we got:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p(x^{(i)} | \\theta)}{\\partial \\Sigma_k} = \\pi_k\\mathcal{N}(x^{(i)} | \\mu_k, \\Sigma_k) * \\big[-\\frac{1}{2}(\\Sigma_k^{-1}-\\Sigma_k^{-1}(x^{(i)}-\\mu_k)(x^{(i)} - \\mu_k)^T\\Sigma_k^{-1}\\big]\n",
    "$$\n",
    "\n",
    "Now consider all samples and log as well, the partial derivative of the log-likelihood with respect to $\\Sigma_k$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\Sigma_k} &=  \\sum\\limits_{i=1}^{m}\\frac{\\partial \\log p(x^{(i)} | \\theta)}{\\partial \\Sigma_k}\\\\\n",
    "&=\\sum\\limits_{i=1}^{m}\\frac{1}{(p(x^{(i)} | \\theta)}\\frac{\\partial p(x^{(i)} | \\theta)}{\\partial \\Sigma_k}\\\\\n",
    "&=\\sum\\limits_{i=1}^{m}\\frac{\\pi_k\\mathcal{N}(x^{(i)} \\mid \\mu_k, \\Sigma_{k})}{\\Sigma_{j=1}^{K} \\pi_j\\mathcal{N}(x^{(i)} \\mid \\mu_j, \\Sigma_j)} * \\big[-\\frac{1}{2}(\\Sigma_k^{-1}-\\Sigma_k^{-1}(x^{(i)}-\\mu_k)(x^{(i)} - \\mu_k)^T\\Sigma_k^{-1})\\big]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Substituting $r^{(i)}_{k}$, we got\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{2}\\sum\\limits_{i=1}^{m}r^{(i)}_{k}(\\Sigma_k^{-1}-\\Sigma_k^{-1}(x^{(i)}-\\mu_k)(x^{(i)} - \\mu_k)^T\\Sigma_k^{-1})\\\\\n",
    "= -\\frac{1}{2}\\Sigma_k^{-1}\\sum\\limits_{i=1}^{m}r^{(i)}_{k} + \\frac{1}{2}\\Sigma_k^{-1}\\big(\\sum\\limits_{i=1}^{m}r^{(i)}_{k}(x^{(i)}-\\mu_k)(x^{(i)} - \\mu_k)^T\\big)\\Sigma_k^{-1}\n",
    "$$\n",
    "\n",
    "Setting this to zero, we obtain:\n",
    "\n",
    "$$\n",
    "N_k\\Sigma_k^{-1} = \\Sigma_k^{-1}\\big(\\sum\\limits_{i=1}^m r^{(i)}_{k}(x^{(i)}-\\mu_k)(x^{(i)} - \\mu_k)^T\\big)\\Sigma_k^{-1}\n",
    "$$\n",
    "\n",
    "By solving for $\\Sigma_k$ we got\n",
    "\n",
    "$$\n",
    "\\Sigma_k = \\frac{1}{N_k}\\sum\\limits_{i=1}^{m}r^{(i)}_{k}(x^{(i)}-\\mu_k)(x^{(i)} - \\mu_k)^T\n",
    "$$\n",
    "\n",
    "**Updating the pi - weight of mixture components**\n",
    "\n",
    "The update of the mixture weights $\\pi_k, k=1,\\cdots,K$ of the GMM is given by:\n",
    "\n",
    "$$ \\pi_k^{new} = \\frac{N_k}{m}$$\n",
    "\n",
    "To prove this:\n",
    "\n",
    "To find the partial derivative, we account for the equality constraint \n",
    "\n",
    "$$\\sum\\limits_{k=1}^K \\pi_k=1$$\n",
    "\n",
    "The Lagrangian $\\mathscr{L}$ is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathscr{L} &= \\mathcal{L} + \\beta\\big(\\sum\\limits_{k=1}^K \\pi_k-1\\big)\\\\\n",
    "&= \\sum\\limits_{i=1}^m \\log \\sum\\limits_{k=1}^K \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k) + \\beta\\big(\\sum\\limits_{k=1}^K \\pi_k-1\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Taking the partial derivative with respect to $\\pi_k$ as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\pi_k} &= \\sum\\limits_{i=1}^m\n",
    "\\frac{\\mathcal{N}(x^{(i)} \\mid \\mu_k, \\Sigma_{k})}{\\Sigma_{j=1}^{K} \\pi_j\\mathcal{N}(x^{(i)} \\mid \\mu_j, \\Sigma_j)} + \\beta \\\\\n",
    "&= \\frac{1}{\\pi_k}\\sum\\limits_{i=1}^m\\frac{\\pi_k\\mathcal{N}(x^{(i)} \\mid \\mu_k, \\Sigma_{k})}{\\Sigma_{j=1}^{K} \\pi_j\\mathcal{N}(x^{(i)} \\mid \\mu_j, \\Sigma_j)} + \\beta\\\\\n",
    "&= \\frac{N_k}{\\pi_k} + \\beta\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Taking the partial derivative with respect to $\\beta$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\beta} = \\sum\\limits_{i=1}^{K} \\pi_k - 1\n",
    "$$\n",
    "\n",
    "Setting both partial derivatives to zero yield\n",
    "\n",
    "$$\n",
    "\\pi_k = -\\frac{N_k}{\\beta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "1 = \\sum\\limits_{i=1}^K\\pi_k\n",
    "$$\n",
    "\n",
    "Using the top formula to solve for the bottom formula:\n",
    "\n",
    "$$    \n",
    "- \\sum\\limits_{i=1}^{m}\\frac{N_k}{\\beta} = 1\\\\\n",
    "= -\\frac{m}{\\beta} = 1\\\\\n",
    "= \\beta = -m\n",
    "$$\n",
    "\n",
    "Substitute $-m$ for $\\beta$ yield\n",
    "\n",
    "$$\n",
    "\\pi_k = \\frac{N_k}{m}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
