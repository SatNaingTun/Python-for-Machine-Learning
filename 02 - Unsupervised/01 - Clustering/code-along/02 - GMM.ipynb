{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition\n",
    "\n",
    "$$\\mathcal{N} (\\mathbf{x} | \\mathbf{\\mu}^{(c)}, \\Sigma^{(c)})= \\frac{1}{(2\\pi)^\\frac{n}{2}\\lvert{\\Sigma^{(c)}}\\rvert^\\frac{1}{2}}e^{(-\\frac{1}{2}(\\mathbf{x} -\\mathbf{\\mu}^{(c)})^T\\Sigma^{(c)-1}(\\mathbf{x} - \\mathbf{\\mu}^{(c)}))}$$\n",
    "\n",
    "<img src = \"../figures/em2.png\" height=400>\n",
    "\n",
    "We shall also further define $\\pi^{(c)}$, which is simply the probability of being in the cluster (think of it as $p(y)$).  Since $\\pi$ is the probability of each cluster, it should sum to 1.\n",
    "\n",
    "$$\\quad 0 \\leq \\pi^{(c)} \\leq 1 \\quad ; \\quad \\sum\\limits_{c=1}^k \\pi^{(c)}=1$$\n",
    "\n",
    "Then, using **Baye's theorem**, we can define the likelihood of a sample $\\mathbf{x}^{(i)}$ belong to a cluster $c$ as simply:\n",
    "\n",
    "$$p(y|x) = p(y) * p(x | y)$$\n",
    "$$p(c | \\mathbf{x}^{(i)}) = \\pi^{(c)} * \\mathcal{N} (\\mathbf{x}^{(i)} | \\mathbf{\\mu}^{(c)}, \\Sigma^{(c)}) $$\n",
    "\n",
    "Finally, based on all the information, GMM defines its objective, i.e., to maximize\n",
    "\n",
    "$$\\prod \\limits_{i=1}^m \\sum_{c=1}^k \\pi^{(c)} \\mathcal{N} (\\mathbf{x}^{(i)} | \\mathbf{\\mu}^{(c)}, \\Sigma^{(c)})$$\n",
    "\n",
    "## Derivatives\n",
    "\n",
    "Our objective function is to find $\\theta$ that maximize the log-likehood $\\mathcal{L}$.  (Here we apply $\\log$ for mathematical stability.)\n",
    "\n",
    "$$\\max_\\theta \\sum\\limits_{i=1}^m \\log \\sum\\limits_{c=1}^k \\pi^{(c)} \\mathcal{N}(\\mathbf{x} | \\mu^{(c)}, \\Sigma^{(c)})$$\n",
    "\n",
    "Our \"normal\" procedure would be to compute the gradient $\\frac{d\\mathcal{L}}{d\\theta}$ and set it to 0.  however, if you try this yourself at home, you will find that it is not possible to find the closed form.\n",
    "\n",
    "### Responsibilities $r$\n",
    "\n",
    "Before anything, let us introduce a quantity that will play a central role in this algorithm: **responsibilities**.  We define the quantity\n",
    "\n",
    "$$ r^{(i)}_{c} = \\frac{\\pi^{(c)}\\mathcal{N}(\\mathbf{x}^{(i)} \\mid \\mu^{(c)}, \\Sigma^{(c)})}{\\Sigma_{c=1}^{k} \\pi^{(c)}\\mathcal{N}(\\mathbf{x}^{(i)} \\mid \\mu^{(c)}, \\Sigma^{(c)})}$$ \n",
    "\n",
    "$r^{(i)}_{c}$ basically gives us $$ \\frac{\\text{Probability of $\\mathbf{x}^{(i)}$ belonging to cluster c}}{\\text{Probability of $\\mathbf{x}^{(i)}$ over all clusters}} $$\n",
    "\n",
    "Note that $r$ for a sample $i$ \n",
    "\n",
    "$$r^{(i)} = r^{(i)}_1, r^{(i)}_2, \\cdots, r^{(i)}_k \\in \\mathbb{R}^k$$\n",
    "\n",
    "$$\\sum\\limits_{c=1}^{k}r^{(i)}_c = 1$$\n",
    "\n",
    "$$ r^{(i)}_c \\geq 0 $$\n",
    "\n",
    "### EM algorithm\n",
    "\n",
    "Notice that $r$ actually depends on **mean, covariance and pi**.  But then, **mean, covariance, and pi** also depends on $r$.  Based on this, we can use EM algorithm, where we can (1) create a random mean, covariance, and pi, (2) calculate $r$, and then repeat 1 and 2 until certain stopping criteria.\n",
    "\n",
    "### Total responsibility $N^{(c)}$\n",
    "\n",
    "By summing all the total responsibility of the $c$ th cluster along all samples, we get $N^{(c)}$.\n",
    "\n",
    "$$N_c = \\sum\\limits_{i=1}^{m}r^{(i)}_c \\in \\mathbb{R}^k $$\n",
    "\n",
    "Note that this value does not necessarily sum to 1.\n",
    "\n",
    "### Updating the mean\n",
    "\n",
    "The update of the mean $\\mu^{(c)}$ is given by:\n",
    "\n",
    "$$ \\mu_\\text{new}^{(c)} = \\frac{\\sum\\limits_{i=1}^{m}r^{(i)}_{c}\\mathbf{x}^{(i)}}{\\sum\\limits_{i=1}^{m}r^{(i)}_{c}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof:**\n",
    "\n",
    "We take a partial derivative of our objective function with respect to the mean parameters $\\mu^{(c)}$. To simplify things, let's perform partial derivative without the log first and only consider one sample.\n",
    "\n",
    "$$\n",
    "\t\\frac{\\partial p(\\mathbf{x}^{(i)} | \\theta)}{\\partial \\mu^{(c)}} = \\sum\\limits_{c=1}^k \\pi^{(c)} \\frac{\\partial \\mathcal{N}(\\mathbf{x}^{(i)} | \\mu^{(c)}, \\Sigma^{(c)})}{\\partial \\mu^{(c)}} = \\pi^{(c)} \\frac{\\partial \\mathcal{N}(\\mathbf{x}^{(i)} | \\mu^{(c)}, \\Sigma^{(c)})}{\\partial \\mu^{(c)}} = \\pi^{(c)}(\\mathbf{x}^{(i)} - \\mu^{(c)})^T \\Sigma^{(c)-1}\\mathcal{N}(\\mathbf{x}^{(i)} | \\mu^{(c)}, \\Sigma^{(c)})\n",
    "$$\n",
    "\n",
    "Now, applying log, since we know the partial derivative of $\\log$ of $x$ is $\\frac{1}{x}$, thus\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mu^{(c)}} =\\sum\\limits_{i=1}^{m} \\frac{\\partial \\log p(\\mathbf{x}^{(i)} | \\theta)}{\\partial \\mu^{(c)}} = \\sum\\limits_{i=1}^{m} \\frac{1}{p(\\mathbf{x}^{(i)} | \\theta)} \\frac{\\partial p(\\mathbf{x}^{(i)} | \\theta) }{\\partial \\mu^{(c)}} = \\sum\\limits_{i=1}^{m}(\\mathbf{x}^{(i)} - \\mu^{(c)})^T \\Sigma^{(c)-1}\\frac{\\pi^{(c)}\\mathcal{N}(\\mathbf{x}^{(i)} \\mid \\mu^{(c)}, \\Sigma^{(c)})}{\\Sigma_{c=1}^{k} \\pi^{(c)}\\mathcal{N}(\\mathbf{x}^{(i)} \\mid \\mu^{(c)}, \\Sigma^{(c)})}\n",
    "$$\n",
    "\n",
    "To simplify, we can substitute $r^{(i)}_{c}$ into the equation, thus\n",
    "\n",
    "$$= \\sum\\limits_{i=1}^{m} r^{(i)}_{c}(\\mathbf{x}^{(i)} - \\mu^{(c)})^T\\Sigma^{(c)-1}$$\n",
    "\n",
    "We can now solve for $\\mu^{(c)}$ so that $\\frac{\\partial \\mathcal{L}}{\\partial \\mu^{(c)}} = 0$ and obtain\n",
    "\n",
    "$$\\sum\\limits_{i=1}^{m} r^{(i)}_{c}(\\mathbf{x}^{(i)} - \\mu^{(c)})^T\\Sigma^{(c)-1} = 0$$\n",
    "\n",
    "Multiply both sides by $\\Sigma$ will cancel out the inverse $\\Sigma$, and move $\\mu^{(c)}$ to another side\n",
    "\n",
    "$$\\sum\\limits_{i=1}^{m} r^{(i)}_{c}\\mathbf{x}^{(i)}  = \\sum\\limits_{i=1}^{m} r^{(i)}_{c}\\mu^{(c)}$$\n",
    "\n",
    "$$\\frac{\\sum\\limits_{i=1}^{m} r^{(i)}_{c}\\mathbf{x}^{(i)} }{\\sum\\limits_{i=1}^{m} r^{(i)}_{c}}  = \\mu^{(c)}$$\n",
    "\n",
    "We can further substitute $N^{(c)}$ so that\n",
    "\n",
    "$$\n",
    "\\frac{1}{N^{(c)}}\\sum\\limits_{i=1}^{m} r^{(i)}_{c}\\mathbf{x}^{(i)} = \\mu^{(c)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the covariances\n",
    "\n",
    "The update of the covariance $\\Sigma^{(c)}$ is given by:\n",
    "\n",
    "$$ \\Sigma^{(c)}_\\text{new} = \\frac{1}{N^{(c)}} \\sum\\limits_{i=1}^{m}r^{(i)}_{c}(\\mathbf{x}^{(i)} - \\mu^{(c)})(\\mathbf{x}^{(i)} - \\mu^{(c)})^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof** \n",
    "\n",
    "We take a partial derivative of our objective function with respect to the Sigma parameters $\\Sigma^{(c)}$. Similarly, to simplify things, let's perform partial derivative without the log first and only consider one sample.\n",
    "\n",
    "$$\n",
    "\t\\frac{\\partial p(\\mathbf{x}^{(i)} | \\theta)}{\\partial \\Sigma^{(c)}} = \\frac{\\partial}{\\partial \\Sigma^{(c)}} \\big(\\pi^{(c)}(2\\pi)^{-\\frac{n}{2}} \\det(\\Sigma^{(c)})^{\\frac{1}{2}}e^{\\big(-\\frac{1}{2}(\\mathbf{x}^{(i)} - \\mu^{(c)})^T\\Sigma^{(c)-1}(\\mathbf{x}^{(i)} - \\mu^{(c)})\\big)\\big)}\n",
    "$$\n",
    "\n",
    "Using derivative multiplication rule, we got\n",
    "\n",
    "$$\n",
    "= \\pi^{(c)}(2\\pi)^{-\\frac{n}{2}}\\big[\\frac{\\partial}{\\partial \\Sigma^{(c)}}\\det(\\Sigma^{(c)})^{-\\frac{1}{2}}exp\\big(-\\frac{1}{2}(\\mathbf{x}^{(i)} - \\mu^{(c)})^T\\Sigma^{(c)-1}(\\mathbf{x}^{(i)} - \\mu^{(c)})) + \\det(\\Sigma^{(c)})^{-\\frac{1}{2}}\\frac{\\partial}{\\partial \\Sigma^{(c)}}exp\\big(-\\frac{1}{2}(\\mathbf{x}^{(i)} - \\mu^{(c)})^T\\Sigma^{(c)}(\\mathbf{x}^{(i)} - \\mu^{(c)})\\big]\n",
    "$$\n",
    "\n",
    "Using this following rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial X}\\det(f(x)) = \\det(f(x))tr\\big(f(x)^{-1}\\frac{\\partial f(x)}{\\partial x}\\big)\n",
    "$$\n",
    "\n",
    "We get that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Sigma^{(c)}}\\det(\\Sigma^{(c)})^{-\\frac{1}{2}} = -\\frac{1}{2}\\det(\\Sigma^{(c)})^{-\\frac{1}{2}}\\Sigma^{(c)-1}\n",
    "$$\n",
    "\n",
    "Using this following rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a^TXb}{\\partial X} = ab^T\n",
    "$$\n",
    "\n",
    "We get that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Sigma^{(c)}}(\\mathbf{x}^{(i)} - \\mu^{(c)})^T\\Sigma^{(c)-1}(\\mathbf{x}^{(i)} - \\mu^{(c)}) = -\\Sigma^{(c)-1}(\\mathbf{x}^{(i)} - \\mu^{(c)})(\\mathbf{x}^{(i)} - \\mu^{(c)})^T\\Sigma^{(c)-1}\n",
    "$$\n",
    "\n",
    "Putting them together, we got:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p(\\mathbf{x}^{(i)} | \\theta)}{\\partial \\Sigma^{(c)}} = \\pi^{(c)}\\mathcal{N}(\\mathbf{x}^{(i)} | \\mu^{(c)}, \\Sigma^{(c)}) * \\big[-\\frac{1}{2}(\\Sigma^{(c)-1}-\\Sigma^{(c)-1}(\\mathbf{x}^{(i)}-\\mu^{(c)})(\\mathbf{x}^{(i)} - \\mu^{(c)})^T\\Sigma^{(c)-1}\\big]\n",
    "$$\n",
    "\n",
    "Now consider all samples and log as well, the partial derivative of the log-likelihood with respect to $\\Sigma^{(c)}$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\Sigma^{(c)}} &=  \\sum\\limits_{i=1}^{m}\\frac{\\partial \\log p(\\mathbf{x}^{(i)} | \\theta)}{\\partial \\Sigma^{(c)}}\\\\\n",
    "&=\\sum\\limits_{i=1}^{m}\\frac{1}{(p(\\mathbf{x}^{(i)} | \\theta)}\\frac{\\partial p(\\mathbf{x}^{(i)} | \\theta)}{\\partial \\Sigma^{(c)}}\\\\\n",
    "&=\\sum\\limits_{i=1}^{m}\\frac{\\pi^{(c)}\\mathcal{N}(\\mathbf{x}^{(i)} \\mid \\mu^{(c)}, \\Sigma^{(c)})}{\\Sigma_{c=1}^{k} \\pi^{(c)}\\mathcal{N}(\\mathbf{x}^{(i)} \\mid \\mu^{(c)}, \\Sigma^{(c)})} * \\big[-\\frac{1}{2}(\\Sigma^{(c)-1}-\\Sigma^{(c)-1}(\\mathbf{x}^{(i)}-\\mu^{(c)})(\\mathbf{x}^{(i)} - \\mu^{(c)})^T\\Sigma^{(c)-1})\\big]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Substituting $r^{(i)}_{c}$, we got\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{2}\\sum\\limits_{i=1}^{m}r^{(i)}_{c}(\\Sigma^{(c)-1}-\\Sigma^{(c)-1}(\\mathbf{x}^{(i)}-\\mu^{(c)})(\\mathbf{x}^{(i)} - \\mu^{(c)})^T\\Sigma^{(c)-1})\\\\\n",
    "= -\\frac{1}{2}\\Sigma^{(c)-1}\\sum\\limits_{i=1}^{m}r^{(i)}_{c} + \\frac{1}{2}\\Sigma^{(c)-1}\\big(\\sum\\limits_{i=1}^{m}r^{(i)}_{c}(\\mathbf{x}^{(i)}-\\mu^{(c)})(\\mathbf{x}^{(i)} - \\mu^{(c)})^T\\big)\\Sigma^{(c)-1}\n",
    "$$\n",
    "\n",
    "Setting this to zero, we obtain:\n",
    "\n",
    "$$\n",
    "N^{(c)}\\Sigma^{(c)-1} = \\Sigma^{(c)-1}\\big(\\sum\\limits_{i=1}^m r^{(i)}_{c}(\\mathbf{x}^{(i)}-\\mu^{(c)})(\\mathbf{x}^{(i)} - \\mu^{(c)})^T\\big)\\Sigma^{(c)-1}\n",
    "$$\n",
    "\n",
    "By solving for $\\Sigma^{(c)}$ we got\n",
    "\n",
    "$$\n",
    "\\Sigma^{(c)} = \\frac{1}{N^{(c)}}\\sum\\limits_{i=1}^{m}r^{(i)}_{c}(\\mathbf{x}^{(i)}-\\mu^{(c)})(\\mathbf{x}^{(i)} - \\mu^{(c)})^T\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the pi\n",
    "\n",
    "The update of the mixture weights $\\pi^{(c)}$:\n",
    "\n",
    "$$ \\pi^{(c)}_\\text{new} = \\frac{N^{(c)}}{m}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**\n",
    "\n",
    "To find the partial derivative, we account for the equality constraint \n",
    "\n",
    "$$\\sum\\limits_{c=1}^k \\pi^{(c)}=1$$\n",
    "\n",
    "The Lagrangian $\\mathscr{L}$ is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathscr{L} &= \\mathcal{L} + \\beta\\big(\\sum\\limits_{c=1}^k \\pi^{(c)}-1\\big)\\\\\n",
    "&= \\sum\\limits_{i=1}^m \\log \\sum\\limits_{c=1}^k \\pi^{(c)} \\mathcal{N}(x | \\mu^{(c)}, \\Sigma^{(c)}) + \\beta\\big(\\sum\\limits_{c=1}^k \\pi^{(c)}-1\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Taking the partial derivative with respect to $\\pi^{(c)}$ as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\pi^{(c)}} &= \\sum\\limits_{i=1}^m\n",
    "\\frac{\\mathcal{N}(\\mathbf{x}^{(i)} \\mid \\mu^{(c)}, \\Sigma^{(c)})}{\\Sigma_{c=1}^{k} \\pi^{(c)}\\mathcal{N}(\\mathbf{x}^{(i)} \\mid \\mu^{(c)}, \\Sigma^{(c)})} + \\beta \\\\\n",
    "&= \\frac{1}{\\pi^{(c)}}\\sum\\limits_{i=1}^m\\frac{\\pi^{(c)}\\mathcal{N}(\\mathbf{x}^{(i)} \\mid \\mu^{(c)}, \\Sigma^{(c)})}{\\Sigma_{c=1}^{k} \\pi^{(c)}\\mathcal{N}(\\mathbf{x}^{(i)} \\mid \\mu^{(c)}, \\Sigma^{(c)})} + \\beta\\\\\n",
    "&= \\frac{N^{(c)}}{\\pi^{(c)}} + \\beta\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Taking the partial derivative with respect to $\\beta$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\beta} = \\sum\\limits_{c=1}^{k} \\pi^{(c)} - 1\n",
    "$$\n",
    "\n",
    "Setting both partial derivatives to zero yield\n",
    "\n",
    "$$\n",
    "\\pi^{(c)} = -\\frac{N^{(c)}}{\\beta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "1 = \\sum\\limits_{c=1}^k\\pi^{(c)}\n",
    "$$\n",
    "\n",
    "Using the top formula to solve for the bottom formula:\n",
    "\n",
    "$$    \n",
    "- \\sum\\limits_{i=1}^{m}\\frac{N^{(c)}}{\\beta} = 1\\\\\n",
    "= -\\frac{m}{\\beta} = 1\\\\\n",
    "= \\beta = -m\n",
    "$$\n",
    "\n",
    "Substitute $-m$ for $\\beta$ yield\n",
    "\n",
    "$$\n",
    "\\pi^{(c)} = \\frac{N^{(c)}}{m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's code!\n",
    "\n",
    "### Step 1: Define k random clusters\n",
    "\n",
    "Define k clusters from k random number of gaussian distribution.  Specifically, for each cluster $c$, randomly initialize parameters mean $\\mathbf{\\mu}^{(c)}$, covariance $\\Sigma^{(c)}$, fraction per class $\\pi^{(c)}$ and responsiblities (likelihood) of each sample $r^{(i)}_{c}$ \n",
    "    \n",
    "Recall that gaussian distribution is parametrized by the mean $\\mathbf{\\mu}$ and the covariance $\\Sigma$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those who forget about covariance matrix, \n",
    "\n",
    "$$\\text{cov}(a) = \\text{var}(a) = \\sum_i^m \\frac{(a^i - \\mu)^2}{m}$$\n",
    "\n",
    "$$\\text{cov}(a, b) = \\sum_i^m \\frac{(a^i - \\mu)(b^i - \\mu)}{m}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Define responsibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: EM-step\n",
    "\n",
    "Repeat until converged:\n",
    "\n",
    "   1. *E-step*: for each sample $\\mathbf{x}^{(i)}$, evaluate responsibilities $r^{(i)}_{c}$ for every data point $\\mathbf{x}^{(i)}$ using \n",
    "\n",
    "$$ r^{(i)}_{c} = \\frac{\\pi^{(c)}\\mathcal{N}(\\mathbf{x}^{(i)} \\mid \\mu^{(c)}, \\Sigma^{(c)})}{\\Sigma_{c=1}^{k} \\pi^{(c)}\\mathcal{N}(\\mathbf{x}^{(i)} \\mid \\mu^{(c)}, \\Sigma^{(c)})}$$\n",
    "\n",
    "   2. *M-step*: for each cluster, update the gaussian distribution of each cluster, i.e., restimate parameters $\\pi^{(c)}, \\mu^{(c)}, \\Sigma^{(c)}, N^{(c)}$ using the updated responsibilites $r^{(i)}_{c}$.\n",
    "\n",
    "$$N^{(c)} = \\sum\\limits_{i=1}^{m}r^{(i)}_c$$\n",
    "\n",
    "$$ \\mu^{(c)}_\\text{new} = \\frac{1}{N^{(c)}} \\sum\\limits_{i=1}^{m}r^{(i)}_{c}\\mathbf{x}^{(i)}$$\n",
    "\n",
    "$$ \\Sigma^{(c)}_\\text{new} = \\frac{1}{N^{(c)}} \\sum\\limits_{i=1}^{m}r^{(i)}_{c}(\\mathbf{x}^{(i)} - \\mu^{(c)})(\\mathbf{x}^{(i)} - \\mu^{(c)})^T$$\n",
    "\n",
    "$$ \\pi^{(c)}_\\text{new} = \\frac{N^{(c)}}{m}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 E-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 M-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Update Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Update mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Update covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
