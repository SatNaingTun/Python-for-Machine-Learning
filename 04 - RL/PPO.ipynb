{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning is a subfield of artificial intelligence (AI) where machines learn by experimenting, somewhat like students learning through trial and error. They aim to make the best decisions to maximize rewards in various scenarios. RL is used in applications like robotics, gaming, and autonomous systems.\n",
    "\n",
    "## Proximal Policy Optimization\n",
    "\n",
    "One of the priminent algorithms are **Proximal Policy Optimization (PPO)**, released by OpenAI in 2017, cited more than 10k times.  It serves as a reliable guide for computers to learn effectively. Instead of making big changes all at once, PPO encourages gradual and more stable improvements, which is crucial for the learning process. PPO finds application in various real-world situations, and we'll explore its principles and significance in more detail.\n",
    "\n",
    "## Terminologies\n",
    "\n",
    "Here are some quick terminologies before we start:\n",
    "\n",
    "- **Agents** - the primary entity interacting with the environment\n",
    "- **Environment** - the primary thing that the agents interact with\n",
    "- **Policy ($\\pi$)**: a policy defines the probability of an action given a state.  The policy guides the agent in selecting actions that maximize its expected cumulative reward over time.\n",
    "- **States / observation** ($s_t$) - a snapshot of the environment at timestep $t$\n",
    "- **Action** ($a_t$) - the possible actions that the agents can perform to the environment at timestep $t$\n",
    "- **Rewards / returns** ($r_t$) - the rewards received each time the agents perform an action at timestep $t$ given state $s$\n",
    "  \n",
    "<!-- \n",
    "## Advanced one...\n",
    "\n",
    "- **Episode** - a single run of the agent interacting with the environment, starting from an initial state ntil a terminal state is reached.\n",
    "- **Rollout** - another name for data collection, so we can train our RL agents on.\n",
    "- **Discounted sum of rewards ($G_t$)** - the amount of immediate rewards and expected future discounted rewards \n",
    "- **Discount Factor ($\\gamma$)**: A parameter that determines the importance of future rewards. A value of 0 indicates that only immediate rewards matter, while a value of 1 considers all future rewards equally.\n",
    "- **Value function** ($V(s)$) - a function that estimates the expected discounted sum of rewards if the agent start at state $s$ until the terminal state, following its current policy\n",
    "- **Exploration / Exploitation** - The trade-off between trying new actions to learn more about the environment (exploration) and exploiting known actions to maximize immediate rewards (exploitation) -->\n",
    "\n",
    "## 1. Vanilla Policy Gradient Methods\n",
    "\n",
    "Vanilla policy gradient methods have an objective function to maximize as:\n",
    "\n",
    "$$\\mathbb{E} (\\log \\pi_\\theta(a_t | s_t) \\hat{A}_t)$$\n",
    "\n",
    "$\\mathbb{E}$ stands for expected value but is simply short hand for empirical average.\n",
    "\n",
    "### 1.1 Actor networks\n",
    "\n",
    "The left side $\\log \\pi_\\theta(a_t | s_t)$ returns the log probability of actions given the current state.  Common way to get is through deep neural network.  This network is commonly called the **actor network**.   It takes in states and output the probability distributions of all actions.\n",
    "\n",
    "The way to train this **actor network** is simply:\n",
    "\n",
    "1. Perform rollout and collect a sequences of states, actions, and rewards\n",
    "2. Once you finish many episodes, \n",
    "   1. Go back to each time step, compute $G_t$ for each timestep where\n",
    "\n",
    "   $$G_t = \\gamma^0 R_{t} + \\gamma^1 R_{t+1} + \\gamma^2 R_{t+2} + \\ldots + \\gamma^{T-t} R_T = \\sum_{k=t}^{T} \\gamma^{k-t} \\cdot R_{k}$$\n",
    "\n",
    "3. Maximize $\\mathbb{E} (\\log \\pi_\\theta(a_t | s_t) G_t)$\n",
    "\n",
    "### 1.2 Advantage function\n",
    "\n",
    "The right side of the equation - the Advantage $\\hat{A}_t$ function is simply calculating the *advantage* of taking a certain action, compared to the average performance.  If $\\hat{A}$ is positive, it means the action taken is generally better than average, while negative means vice versa.  If it is zero, it means that the action taken is no better than average.\n",
    "\n",
    "The equation is\n",
    "\n",
    "$$\\hat{A}_t = G_t - V(s)$$\n",
    "\n",
    "where $V(s)$ is the **state value function** or the **value function** that calculates the average discounted sum of rewards whenever the agent stay in this state until the end.  It is commonly calculated using the **critic network** (or sometimes the value network).\n",
    "\n",
    "### 1.3. Critic network\n",
    "\n",
    "It is a neural network that takes in states $(s)$ and output its corresponding value $(V(s))$.  The way to train this network is simply:\n",
    "\n",
    "1. Perform rollout and collect a sequences of states, actions, and rewards\n",
    "2. Once you finish many episodes, \n",
    "   1. Go back to each time step, compute $G_t$ for each timestep\n",
    "3. Minimize $\\mathbb{E}(G_t - V(s))^2$\n",
    "\n",
    "### In summary\n",
    "\n",
    "In summary, it's basically maximizing the probability of actions, multiplied by its relative rewards compared to average.\n",
    "\n",
    "$$\\mathbb{E} \\underbrace{(\\log \\pi_\\theta(a_t | s_t)}_{\\text{prob. of actions}} \\underbrace{\\hat{A}_t}_{\\text{relative rewards}})$$\n",
    "\n",
    "## 2. Trust Region Methods\n",
    "\n",
    "The vanilla policy gradient methods often suffer from too large policy updates, causing it fail to find the solution due to the large solution space. \n",
    "\n",
    "**Trust Region Policy Gradient (TRPO)** modifies the objective function to maximize to\n",
    "\n",
    "$$\\mathbb{E}(\\frac{\\log \\pi_\\theta(a_t | s_t)}{\\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\hat{A}_t) - \\beta \\text{KL}(\\pi_{\\theta_{\\text{old}}}( \\cdot | s_t), \\pi_{\\theta}( \\cdot | s_t))$$\n",
    "\n",
    "The first term calculates the **ratio** instead of the probability, which would help the model to find meaningful updates.  Larger difference, the better.  However, too large updates can cause a lot of problem, thus we constrain the difference between old policy and new policy using KL divergence.\n",
    "\n",
    "Note: For those who don't know what is KL divergence, you can easily search up the equation, but it is a very simple equation of $P \\log \\frac{P}{Q}$ which simply measures how different is two distributions $P$ and $Q$.\n",
    "\n",
    "## 3. Proximal Policy Methods\n",
    "\n",
    "TRPO still suffers from choosing the right $\\beta$ which varies from task to task.  \n",
    "\n",
    "**Proximal Policy Policy Gradient (PPO)** modifies the objective function to maximize to\n",
    "\n",
    "$$\\mathbb{E}( \n",
    "   \\min(\n",
    "   \\frac{\\log \\pi_\\theta(a_t | s_t)}{\\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\hat{A}_t , \n",
    "   \\text{clip}(\\frac{\\log \\pi_\\theta(a_t | s_t)}{\\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon)\\hat{A}_t)           ) $$\n",
    "\n",
    "Although this looks very difficult, it simply bounds the updates within $\\epsilon$ which is specified commonly as 0.2. \n",
    "\n",
    "Let's look at the effect closely via this picture:\n",
    "\n",
    "<img src = \"figures/clip.png\" height=\"300\">\n",
    "\n",
    "Here, the x-axis is the policy ratio, and the y-axis is simply the objective function we just defined.  \n",
    "When $A > 0$, it restricts the rewards to $1 + \\epsilon$.  When $A < 0$, it restricts the rewards to $1 - \\epsilon$.\n",
    "\n",
    "### 3.1 Actor loss function\n",
    "\n",
    "Based on what we learn, the **actor** loss function of PPO is simply\n",
    "\n",
    "$$J(\\theta) = -\\min(\n",
    "   \\frac{\\log \\pi_\\theta(a_t | s_t)}{\\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\hat{A}_t , \n",
    "   \\text{clip}(\\frac{\\log \\pi_\\theta(a_t | s_t)}{\\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon)\\hat{A}_t) $$\n",
    "\n",
    "To encourage exploration, it adds the **entropy** term\n",
    "\n",
    "$$J(\\theta) = -\\min(\n",
    "   \\frac{\\log \\pi_\\theta(a_t | s_t)}{\\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\hat{A}_t , \n",
    "   \\text{clip}(\\frac{\\log \\pi_\\theta(a_t | s_t)}{\\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon)\\hat{A}_t) - \\lambda S[\\pi_\\theta](s_t)$$\n",
    "\n",
    "Here, we want *high* entropy, thus we put minus in front for a minimization problem.  $\\lambda$ is simply a coefficient to control this entropy bonus.\n",
    "\n",
    "#### Entropy\n",
    "\n",
    "To understand how entropy helps exploration, a simple example of **entropy** is:\n",
    "\n",
    "Suppose you have three actions, A1, A2, and A3. The probabilities to each action as follows:\n",
    "\n",
    "- Probability of selecting A1: $P(A1) = 0.9$\n",
    "- Probability of selecting A2: $P(A2) = 0.05$\n",
    "- Probability of selecting A3: $P(A3) = 0.05$\n",
    "\n",
    "The entropy of this policy can be calculated as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "S &= -\\sum_{i=1}^{3} P(A_i) \\ln(P(A_i)) \\\\\n",
    "S &= -(0.9 \\ln(0.9) + 0.05 \\ln(0.05) + 0.05 \\ln(0.05)) \\\\\n",
    "S &\\approx 0.394\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's change the probabilities to \n",
    "\n",
    "- Probability of selecting A1: $P(A1) = 0.4$\n",
    "- Probability of selecting A2: $P(A2) = 0.3$\n",
    "- Probability of selecting A3: $P(A3) = 0.3$\n",
    "\n",
    "The entropy $S$ will be 1.08.\n",
    "\n",
    "Thus higher entropy encourages the model to explore more actions, instead of just making one action very prominent.  However, it is important to note that we should balance exploration and exploitation by putting a coefficient in front of this entropy to control how much we want to explore.\n",
    "\n",
    "### 3.2 Critic loss function\n",
    "\n",
    "The critic loss function is simply\n",
    "\n",
    "$$J(\\theta) = (G_t - V(s))^2$$\n",
    "\n",
    "That's it!  Now let's look at the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms and code\n",
    "\n",
    "1. Initialize actor and critic network\n",
    "2. Collect data\n",
    "   1. Initialize environment and its states\n",
    "   2. Let the agent interact with the environment\n",
    "      1. Store the state, rewards, actions, next states, and log probability of the action into a list\n",
    "   3. Once you finish collecting these states, rewards, etc.\n",
    "      1. Compute G_t, i.e., the discounted rewards\n",
    "3. Calculate probability\n",
    "   1. $\\displaystyle\\frac{\\pi}{\\pi_\\text{old}} \\hat{A}_t$\n",
    "   2. $\\text{clip}(\\displaystyle\\frac{\\pi}{\\pi_\\text{old}}, 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t$\n",
    "4. Calculate loss\n",
    "   1. Actor loss = - minimum of the 3.1 and 3.2 - entropy bonus\n",
    "   2. Critic loss = $(G_t - V)^2$\n",
    "5. Backpropagate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone neural network for actor and critic networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(in_dim, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, out_dim)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        if isinstance(states, np.ndarray):\n",
    "            states = torch.tensor(states, dtype = torch.float)\n",
    "                    \n",
    "        activation1 = F.relu(self.layer1(states))\n",
    "        activation2 = F.relu(self.layer2(activation1))\n",
    "        out         = self.layer3(activation2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.optim import Adam\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env):\n",
    "        self._init_params()\n",
    "        \n",
    "        #extract info from environment\n",
    "        self.env = env\n",
    "        self.states_dim = env.observation_space.shape[0]\n",
    "        self.act_dim    = env.action_space.shape[0]\n",
    "        \n",
    "        ## STEP 1\n",
    "        #input is state for both actor and critic networks\n",
    "        #output is a value for critic networks, and action distribution for actor networks \n",
    "        self.actor  = FeedForwardNN(self.states_dim, self.act_dim) \n",
    "        self.critic = FeedForwardNN(self.states_dim, 1)\n",
    "        \n",
    "        ##this is for sampling actions when collecting data\n",
    "        self.cov_var = torch.full(size = (self.act_dim, ), fill_value=0.5)\n",
    "        self.cov_mat = torch.diag(self.cov_var)  #basically every action has a probabiliy of 0.5\n",
    "        \n",
    "        self.actor_optim = Adam(self.actor.parameters(), lr=self.lr)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=self.lr)\n",
    "    \n",
    "    def _init_params(self):\n",
    "        torch.manual_seed(999)  #just for reproducibility\n",
    "        self.timesteps_per_batch = 4800\n",
    "        self.max_timesteps_per_episode = 1600\n",
    "        self.gamma = 0.95\n",
    "        self.n_updates_per_iteration = 5\n",
    "        self.clip = 0.2\n",
    "        self.lr = 0.005\n",
    "        self.entropy_weight = 0.05 #higher means more exploration; we can set it very low for pendulum because it's a very simple problem\n",
    "    \n",
    "    ## STEP 2\n",
    "    def collect_data(self):\n",
    "        #rollout\n",
    "        batch_states    = [] #shape: (number of timesteps per batch, states_dim)\n",
    "        batch_acts      = [] #shape: (number of timesteps per batch, act_dim)\n",
    "        batch_log_probs = [] #(number of timesteps per batch, )\n",
    "        batch_rewards   = [] #(number of episodes, number of timesteps per episode)\n",
    "        batch_discounted_rewards = [] #(number of timesteps per batch, )\n",
    "        batch_lens      = [] #(number of episodes, )\n",
    "        \n",
    "        #Number of timesteps run so far this batch\n",
    "        t = 0\n",
    "        ep_rewards = []\n",
    "        \n",
    "        #batch means one batch of data we collect, which can span multiple episodes\n",
    "        #one episode means you start the env, until you reach the terminal state\n",
    "        \n",
    "        while t < self.timesteps_per_batch:  #30\n",
    "            \n",
    "            #Rewards this episode\n",
    "            ep_rewards = []\n",
    "            \n",
    "            states = self.env.reset()[0]  ## STEP 2.1\n",
    "            done   = False\n",
    "            \n",
    "            ## STEP 2.2\n",
    "            for ep_t in range(self.max_timesteps_per_episode):\n",
    "                t += 1\n",
    "                \n",
    "                #collect states\n",
    "                batch_states.append(states)\n",
    "                \n",
    "                action, log_prob = self.get_action(states)    \n",
    "                states, rewards, done, _, _ = self.env.step(action)\n",
    "                \n",
    "                #collect reward, action, and log prob\n",
    "                ep_rewards.append(rewards)                \n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "            batch_lens.append(ep_t + 1)           \n",
    "            batch_rewards.append(ep_rewards)\n",
    "        \n",
    "        #convert to tensor; note that converting the list first to np array then to tensor is much faster\n",
    "        batch_states    = torch.tensor(np.array(batch_states), dtype=torch.float)\n",
    "        batch_acts      = torch.tensor(np.array(batch_acts), dtype=torch.float)\n",
    "        batch_log_probs = torch.tensor(np.array(batch_log_probs), dtype=torch.float)\n",
    "\n",
    "        ## STEP 2.3\n",
    "        #compute G_t\n",
    "        batch_discounted_rewards = self.compute_discounted_rewards(batch_rewards)\n",
    "        \n",
    "        return batch_states, batch_acts, batch_log_probs, batch_discounted_rewards, batch_lens\n",
    "                \n",
    "    def fit(self, total_timesteps):\n",
    "        t = 0 # Timesteps simulated until now\n",
    "        i = 0\n",
    "        actor_losses  = [] #for reporting\n",
    "        critic_losses = []\n",
    "        discounted_rewards = []\n",
    "        \n",
    "        while t < total_timesteps:\n",
    "                        \n",
    "            batch_states, batch_acts, batch_log_probs, batch_discounted_rewards, batch_lens = self.collect_data()\n",
    "                        \n",
    "            t += np.sum(batch_lens)\n",
    "            i += 1\n",
    "                    \n",
    "            # Calculate V\n",
    "            V, _ , _ = self.predict(batch_states, batch_acts)\n",
    "\n",
    "            # Calculate advantage\n",
    "            A_k = batch_discounted_rewards - V.detach()\n",
    "            \n",
    "            # For faster convergence\n",
    "            A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "            \n",
    "            for _ in range(self.n_updates_per_iteration):\n",
    "                V, curr_log_probs, entropy = self.predict(batch_states, batch_acts)\n",
    "                ratios = torch.exp(curr_log_probs - batch_log_probs) #log ratio become minus\n",
    "                \n",
    "                # Calculate surrogate losses\n",
    "                surr1 = ratios * A_k\n",
    "                surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k\n",
    "                actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "                entropy_loss = entropy.mean()\n",
    "                actor_loss = actor_loss - self.entropy_weight * entropy_loss\n",
    "                critic_loss = nn.MSELoss()(batch_discounted_rewards, V)\n",
    "                \n",
    "                discounted_rewards.append(batch_discounted_rewards.mean())\n",
    "                \n",
    "                actor_losses.append(actor_loss.detach())\n",
    "                critic_losses.append(critic_loss.detach())\n",
    "                \n",
    "                # Backprop\n",
    "                self.actor_optim.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optim.step()\n",
    "                \n",
    "                self.critic_optim.zero_grad()    \n",
    "                critic_loss.backward()    \n",
    "                self.critic_optim.step()\n",
    "                \n",
    "            self.print_summary(i, t, discounted_rewards, critic_losses, actor_losses)\n",
    "                \n",
    "    def get_action(self, states):\n",
    "        mean = self.actor(states)\n",
    "        dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        #detach from computational graph\n",
    "        return action.detach().numpy(), log_prob.detach()\n",
    "    \n",
    "    def compute_discounted_rewards(self, batch_rewards):\n",
    "        # batch_rewards: shape (number of episodes, number of timesteps per episode)\n",
    "        batch_discounted_rewards = []  #shape: (num of timesteps in batch)\n",
    "                        \n",
    "        # Iterate through each episode backwards to maintain same order in batch_discounted_rewards\n",
    "        for episode_reward in reversed(batch_rewards):\n",
    "        \n",
    "            discounted_reward = 0\n",
    "            for reward in reversed(episode_reward):\n",
    "                discounted_reward = reward + discounted_reward * self.gamma\n",
    "                batch_discounted_rewards.insert(0, discounted_reward)\n",
    "                \n",
    "        batch_discounted_rewards = torch.tensor(batch_discounted_rewards, dtype=torch.float)\n",
    "        \n",
    "        return batch_discounted_rewards\n",
    "    \n",
    "    def predict(self, batch_states, batch_acts):\n",
    "        # Query critic network for a value V for each state in batch_states.\n",
    "        V = self.critic(batch_states).squeeze()\n",
    "        \n",
    "        mean = self.actor(batch_states)\n",
    "        dist = MultivariateNormal(mean, self.cov_mat)\n",
    "                \n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "        \n",
    "        return V, log_probs, dist.entropy()\n",
    "    \n",
    "    def print_summary(self, i, t, discounted_rewards, critic_losses, actor_losses):\n",
    "        avg_discounted_rewards  = np.mean([rewards.float().mean() for rewards in discounted_rewards])\n",
    "        avg_actor_loss  = np.mean([losses.float().mean() for losses in actor_losses])\n",
    "        avg_critic_loss = np.mean([losses.float().mean() for losses in critic_losses])\n",
    "        \n",
    "        if(i+1) % 10 == 0:\n",
    "            print(f\"#{i+1:3.0f} | Timesteps: {t:7.0f} |  Critic Loss: {avg_critic_loss:10.3f} | Actor Loss: {avg_actor_loss:10.6f} | Dis. Rewards: {avg_discounted_rewards:5.3f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 10 | Timesteps:   43200 |  Critic Loss:   5708.219 | Actor Loss:  -0.049271 | Dis. Rewards: -99.159\n",
      "# 20 | Timesteps:   91200 |  Critic Loss:   3858.631 | Actor Loss:  -0.051775 | Dis. Rewards: -104.067\n",
      "# 30 | Timesteps:  139200 |  Critic Loss:   2805.850 | Actor Loss:  -0.052878 | Dis. Rewards: -100.241\n",
      "# 40 | Timesteps:  187200 |  Critic Loss:   2229.158 | Actor Loss:  -0.053582 | Dis. Rewards: -94.599\n",
      "# 50 | Timesteps:  235200 |  Critic Loss:   1883.683 | Actor Loss:  -0.054107 | Dis. Rewards: -82.807\n",
      "# 60 | Timesteps:  283200 |  Critic Loss:   1606.653 | Actor Loss:  -0.054269 | Dis. Rewards: -69.622\n",
      "# 70 | Timesteps:  331200 |  Critic Loss:   1433.729 | Actor Loss:  -0.053816 | Dis. Rewards: -61.903\n",
      "# 80 | Timesteps:  379200 |  Critic Loss:   1265.802 | Actor Loss:  -0.053957 | Dis. Rewards: -54.653\n",
      "# 90 | Timesteps:  427200 |  Critic Loss:   1129.903 | Actor Loss:  -0.054048 | Dis. Rewards: -48.825\n",
      "#100 | Timesteps:  475200 |  Critic Loss:   1019.875 | Actor Loss:  -0.054126 | Dis. Rewards: -44.136\n"
     ]
    }
   ],
   "source": [
    "#pip install gymnasium\n",
    "#brew install swig\n",
    "#pip install box2d-py\n",
    "\n",
    "import gymnasium as gym\n",
    "import pickle\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "model = PPO(env)\n",
    "model.fit(500000)\n",
    "\n",
    "filename = 'model/pendulumv1'\n",
    "with open(f'{filename}.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    1 | Angle:  0.239 | Reward: -0.07073\n",
      "Iteration    2 | Angle:  0.221 | Reward: -0.06798\n",
      "Iteration    3 | Angle:  0.201 | Reward: -0.06386\n",
      "Iteration    4 | Angle:  0.185 | Reward: -0.05672\n",
      "Iteration    5 | Angle:  0.173 | Reward: -0.04511\n",
      "Iteration    6 | Angle:  0.168 | Reward: -0.03514\n",
      "Iteration    7 | Angle:  0.170 | Reward: -0.02948\n",
      "Iteration    8 | Angle:  0.163 | Reward: -0.03301\n",
      "Iteration    9 | Angle:  0.162 | Reward: -0.02839\n",
      "Iteration   10 | Angle:  0.158 | Reward: -0.02778\n",
      "Iteration   11 | Angle:  0.146 | Reward: -0.02854\n",
      "Iteration   12 | Angle:  0.125 | Reward: -0.03053\n",
      "Iteration   13 | Angle:  0.113 | Reward: -0.03334\n",
      "Iteration   14 | Angle:  0.103 | Reward: -0.01930\n",
      "Iteration   15 | Angle:  0.089 | Reward: -0.01543\n",
      "Iteration   16 | Angle:  0.080 | Reward: -0.01586\n",
      "Iteration   17 | Angle:  0.070 | Reward: -0.01002\n",
      "Iteration   18 | Angle:  0.069 | Reward: -0.00928\n",
      "Iteration   19 | Angle:  0.065 | Reward: -0.00549\n",
      "Iteration   20 | Angle:  0.068 | Reward: -0.00558\n",
      "Iteration   21 | Angle:  0.079 | Reward: -0.00547\n",
      "Iteration   22 | Angle:  0.084 | Reward: -0.01198\n",
      "Iteration   23 | Angle:  0.091 | Reward: -0.00827\n",
      "Iteration   24 | Angle:  0.097 | Reward: -0.01055\n",
      "Iteration   25 | Angle:  0.101 | Reward: -0.01184\n",
      "Iteration   26 | Angle:  0.110 | Reward: -0.01067\n",
      "Iteration   27 | Angle:  0.119 | Reward: -0.01588\n",
      "Iteration   28 | Angle:  0.124 | Reward: -0.01925\n",
      "Iteration   29 | Angle:  0.133 | Reward: -0.01600\n",
      "Iteration   30 | Angle:  0.132 | Reward: -0.02525\n",
      "Iteration   31 | Angle:  0.142 | Reward: -0.01805\n",
      "Iteration   32 | Angle:  0.142 | Reward: -0.02798\n",
      "Iteration   33 | Angle:  0.134 | Reward: -0.02332\n",
      "Iteration   34 | Angle:  0.125 | Reward: -0.02126\n",
      "Iteration   35 | Angle:  0.113 | Reward: -0.02011\n",
      "Iteration   36 | Angle:  0.101 | Reward: -0.01894\n",
      "Iteration   37 | Angle:  0.088 | Reward: -0.01618\n",
      "Iteration   38 | Angle:  0.087 | Reward: -0.01523\n",
      "Iteration   39 | Angle:  0.085 | Reward: -0.00778\n",
      "Iteration   40 | Angle:  0.089 | Reward: -0.00740\n",
      "Iteration   41 | Angle:  0.087 | Reward: -0.00965\n",
      "Iteration   42 | Angle:  0.092 | Reward: -0.00780\n",
      "Iteration   43 | Angle:  0.102 | Reward: -0.00925\n",
      "Iteration   44 | Angle:  0.100 | Reward: -0.01832\n",
      "Iteration   45 | Angle:  0.100 | Reward: -0.01030\n",
      "Iteration   46 | Angle:  0.097 | Reward: -0.01085\n",
      "Iteration   47 | Angle:  0.090 | Reward: -0.01082\n",
      "Iteration   48 | Angle:  0.086 | Reward: -0.01012\n",
      "Iteration   49 | Angle:  0.076 | Reward: -0.00943\n",
      "Iteration   50 | Angle:  0.069 | Reward: -0.00954\n",
      "Iteration   51 | Angle:  0.066 | Reward: -0.00698\n",
      "Iteration   52 | Angle:  0.070 | Reward: -0.00509\n",
      "Iteration   53 | Angle:  0.076 | Reward: -0.00540\n",
      "Iteration   54 | Angle:  0.072 | Reward: -0.01122\n",
      "Iteration   55 | Angle:  0.067 | Reward: -0.00617\n",
      "Iteration   56 | Angle:  0.064 | Reward: -0.00550\n",
      "Iteration   57 | Angle:  0.057 | Reward: -0.00554\n",
      "Iteration   58 | Angle:  0.051 | Reward: -0.00570\n",
      "Iteration   59 | Angle:  0.050 | Reward: -0.00416\n",
      "Iteration   60 | Angle:  0.044 | Reward: -0.00326\n",
      "Iteration   61 | Angle:  0.048 | Reward: -0.00412\n",
      "Iteration   62 | Angle:  0.050 | Reward: -0.00301\n",
      "Iteration   63 | Angle:  0.046 | Reward: -0.00409\n",
      "Iteration   64 | Angle:  0.058 | Reward: -0.00691\n",
      "Iteration   65 | Angle:  0.060 | Reward: -0.01210\n",
      "Iteration   66 | Angle:  0.059 | Reward: -0.00424\n",
      "Iteration   67 | Angle:  0.049 | Reward: -0.00566\n",
      "Iteration   68 | Angle:  0.044 | Reward: -0.00632\n",
      "Iteration   69 | Angle:  0.035 | Reward: -0.00350\n",
      "Iteration   70 | Angle:  0.028 | Reward: -0.00403\n",
      "Iteration   71 | Angle:  0.016 | Reward: -0.00346\n",
      "Iteration   72 | Angle:  0.004 | Reward: -0.00619\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import pickle\n",
    "\n",
    "filename = 'model/pendulumv1'\n",
    "\n",
    "with open(f'{filename}.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "env = gym.make('Pendulum-v1', render_mode='human')\n",
    "num_episodes = 1\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    i = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "\n",
    "        action, log_probabilities = model.get_action(state)\n",
    "        next_state, reward, done, truncated, info = env.step(action)    \n",
    "        \n",
    "        angle = np.arctan2(next_state[1], next_state[0])\n",
    "        angle_threshold = 0.005\n",
    "        if abs(angle) < angle_threshold:\n",
    "            done = True\n",
    "                       \n",
    "        state = next_state\n",
    "        \n",
    "        i += 1\n",
    "        #the more negative the rewards, the farther it is from the upright position.\n",
    "        print(f\"Iteration {i: 4.0f} | Angle: {angle:6.3f} | Reward: {reward:3.5f}\")\n",
    "\n",
    "env.close() #env.close() won't close the window; just restart the kernel and it will close the window"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
